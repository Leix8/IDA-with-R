---
title: 'Intro to R Workshop: Session 4'
subtitle: UCI Data Science Initiative
date: "October 20, 2017"
#author: Chris Galbraith
output: slidy_presentation
smaller: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Session 4 Agenda
+ Logistic Regression

    + Brief Intro to Generalized Linear Models (GLMs)
    + Fitting Models for Binary Response Data
    + Interpretation
    + Diagnostics
    + Prediction


## O-Ring Data
+ On January 28, 1986, the USA Space Shuttle Challenger exploded 73 seconds into flight, killing all 7 crew members. The explosion was traced to failure of O-ring seals in the solid rocket booster at liftoff.

+ It was hypothesized that there is a inverse relationship between O-ring failure and temperature. 

+ The night before the flight, engineers had to make a decision whether or not to lauch Challenger the next day with a forecasted temperature of 31 degrees Fahrenheit.

+ The engineers had data available on 23 previous launches to aid in their decision with the following varaibles 

    + Number of O-rings at risk on a given flight 
    + Number experiencing thermal distress 
    + Launch temperature (degrees F) 
    + Leak-check pressure (psi) 
    + Temporal order of flight


## O-Ring Data, contd.
More information about the O-ring data is available [here](https://archive.ics.uci.edu/ml/datasets/Challenger+USA+Space+Shuttle+O-Ring).

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
oring <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/space-shuttle/o-ring-erosion-only.data")
names(oring) <- c("n_risk", "n_fail", "temp", "psi", "order")
head(oring)
summary(oring)
```


## Visualizing the O-Ring Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
oring <- oring[order(oring$temp), ]  # sort by ascending temp
# some flights have the same number of failures & launch temp, so make point size vary 
pt.size <- rep(1, nrow(oring))
dups <- oring[duplicated(cbind(oring$temp, oring$n_fail)), c("n_fail", "temp")]
pt.size[as.numeric(rownames(dups))] <- 2
trips <- dups[duplicated(dups), ]
pt.size[as.numeric(rownames(trips))] <- 3
plot(oring$temp, oring$n_fail, pch = 20, cex = pt.size, 
     xlab = "Temperature (deg F)", ylab = "Number of Failures")  
```


## Naive Analysis of the O-Ring Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
## Fit a linear model
linearMod <- lm(n_fail ~ temp, data = oring)
summary(linearMod)
```


## Naive Analysis Results
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
## Predict the output for the Challenger launch day
launchDay <- data.frame(temp = 31)
predict(linearMod, launchDay, interval="predict")  # point estimate & 95% CI

## Plot the fit extrapolating to launch day
plot(oring$temp, oring$n_fail, pch = 20, cex = pt.size/2, 
     xlab = "Temperature (deg F)", ylab = "Num O-Ring Failures",
     xlim = c(30, 85), ylim = c(0, 5))
abline(reg = linearMod, col = "red", lwd = 2)
points(x=31, y=2.52, pch=4, col="black", lwd=2)
x <- seq(25,90,.1)
pred <- predict(linearMod, data.frame(temp=x), interval="predict")
lines(x, pred[,2], lty=2, col="red")
lines(x, pred[,3], lty=2, col="red")
```


## Generalized Linear Models (GLMs)
+ Linear regression clearly isn't the proper technique for this type of data, but GLMs can overcome these issues.

+ Flexible extension of linear regression that allows response variable to have non-Gaussian error distributions.

+ Instead of modeling the mean $\mu$ of the response variable directly as a linear combination of the predictors, we model some function $g(\mu)$.


## GLMs in R
asdf


##  Logistic Regression - O-Ring Data
+ `glm()` is used to fit generalized linear models (logistic regression in this example).
+ Logistic regression is used to model the probability of a binary outcome.
+ For instance, say we want to predict female labor force participation using the `Mroz` data.

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
library(car)
data(Mroz) # load Mroz data
str(Mroz)
```


## Logistic Regression - Data Description
+ lfp: female labor-force participation (factor with levels no/yes)
+ k5: number of children 5 years old or younger
+ k618: number of children 6 to 18 years old
+ age: in years
+ wc: wife's college attendance (factor with levels no/yes)
+ hc: husband's college attendance (factor with levels no/yes)
+ lwg: log expected wage rate
    + for women in the labor force, the actual wage rate
    + for women not in the labor force, an imputed value 
+ inc: family income exclusive of wife's income


## Logistic Regression - Exploratory Data Analysis (EDA)
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(Mroz)
```


## Logistic Regression - EDA
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
barplot(table(Mroz$lfp), col = "blue", 
        main = "Count of Females by Labor Force Participation",
        xlab = "Labor Force Participation Status", ylab = "Number of Females")
```


## Logistic Regression - EDA
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
counts <- table(Mroz$lfp, Mroz$k5)
barplot(counts, col = c("blue","red"),
        main = "Labor Force Participation by # of Children < 5",
        xlab = "# of Children < 5", ylab = "Count",
        legend = rownames(counts))
```


## Logistic Regression - EDA
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
counts <- table(Mroz$lfp, Mroz$k5)
barplot(counts, col = c("blue","red"),
        main = "Labor Force Participation by # of Children < 5",
        xlab = "# of Children < 5", ylab = "Count",
        legend = rownames(counts), beside = T)
```


## Logistic Regression - EDA
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
plot(Mroz$age, Mroz$lwg, col = ifelse(Mroz$lfp == "yes", "red", "blue"),
     main = "Age vs. Log Wage by LFP", xlab = "Age (Years)", ylab = "Wage")
legend(x = 50, y = -1.5, legend = c("in LF", "Out of LF"),
       fill = c("red", "blue"), bty = "n")
```


## Logistic Regression - Model Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
fitLogistic <- glm(lfp ~ k5 + age, family=binomial(logit), data=Mroz)
fitLogistic
```


## Logistic Regression - Model Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(fitLogistic)
```


## Logistic Regression - Model Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
names(fitLogistic)
```


## Logistic Regression - Exponentiated CIs
+ 95% CI for exp(coefficients) (profile likelihood method)
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
exp(confint(fitLogistic, level=0.95))
```

+ 95% CI for exp(coefficients) (Wald confidence interval)
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
exp(confint.default(fitLogistic, level=0.95))
```


## Logistic Regression - Predictions
To get probabilities, be sure to use type = "response"
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
head(Mroz,1)
XB <- as.vector(head(predict(fitLogistic),1))
XB
p <- as.vector(head(predict(fitLogistic, type = "response"),1))
p
exp(XB)/(1 + exp(XB))  # inverse logit function
```


## Logistic Regression - Updates
+ Update model by adding 'inc' and 'lwg'
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
fitLogistic2 = update(fitLogistic, . ~ . + inc + lwg, data=Mroz)
```

+ After update
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
fitLogistic2
```


## Model Comparison
+ Use change of deviance of fitted model
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
anova(fitLogistic, fitLogistic2, test='LRT')
```


## End of Session 4
Make sure you submit course evaluation surveys!



