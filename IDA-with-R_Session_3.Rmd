---
title: "Intro to Data Analysis with R: Session 3"
subtitle: UCI Data Science Initiative
date: "October 20, 2017"
#author: "Chris Galbraith"
output: slidy_presentation
smaller: yes
---

```{r, include=FALSE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("./dev/IDA-with-R")
```


## Session 3 Agenda

1. Statistical Distributions (very brief) 

2. T-Tests

3. Linear Regression

    + Fitting Models
    + Interpretation
    + Diagnostics
    + Prediction


## Statistical Distributions in R:

+ R has many built-in statistical distributions
    + e.g., binomial, poisson, normal, chi square, ...

+ Each distribution in R has four functions:
    + These functions begin with a "d", "p", "q", or "r" and are followed by the name of the distribution

+ ```d<dist>()```: evaluates the probability density/mass function at a given value
+ ```r<dist>()```: generates random numbers
+ ```p<dist>()```: returns the cumulative distribution function (CDF) for a given quantile
+ ```q<dist>()```: returns the quantile for a given probability


## Standard Normal Distribution

+ Calculate the value of the probability density function at $X = 0$
```{r echo=TRUE}  
str(dnorm) # normal pdf
dnorm(x = 0, mean = 0, sd = 1)
```


## Standard Normal Distribution
```{r echo=TRUE, fig.height = 4.5, fig.align='center'}  
x <- seq(from = -3, to = 3, by = 0.05)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l")
```

## Standard Normal Distribution

+ Generate 10 independent random numbers from a standard normal distribution
```{r echo=TRUE}  
str(rnorm) # generate random number from normal dist
rnorm(10, mean = 0, sd = 1)
```


## Standard Normal Distribution

+ Calculate the probability that $X \leq 0$
```{r echo=TRUE}  
str(pnorm) # normal CDF
pnorm(0, mean = 0, sd = 1) # Pr[X <= 0] = ?
```


## Standard Normal Distribution

+ Find the value for which the CDF = 0.975
```{r echo=TRUE}  
str(qnorm) # normal quantile func
qnorm(0.975, mean = 0, sd = 1) # PR[X <= ?] = 0.975
```


## T-Tests
T-tests can be used to draw statistical conclusions about parameters of interest in the data

+ Is the mean of this data different from zero (or another number)?

+ Are the means of two data sets different from one another?

+ Is the regression slope coefficient different from zero?

T-tests can be categorized into two groups:

1. One-sample t-test

2. Two-sample t-test


##  One-Sample T-Test (Create Data)
```{r echo=TRUE}
set.seed(123)
oneSampData <- rnorm(100, mean = 0, sd = 1)

mean(oneSampData)
sd(oneSampData)
```


##  One-Sample T-Test ($H_0: \mu = 0$)
```{r echo=TRUE}
oneSampTest.0 <- t.test(oneSampData) 
oneSampTest.0
```

```{r echo=TRUE}
names(oneSampTest.0) 
oneSampTest.0$statistic
oneSampTest.0$estimate
```  


##  One-Sample T-Test ($H_0: \mu = a$)
```{r echo=TRUE}
a <- 0.3
oneSampTest.mu <- t.test(oneSampData, mu = a)
oneSampTest.mu
```  


##  Two-Sample T-Test
Two sample t-tests are categorized into 3 groups:

  + T-Test with equal variances
  
  + T-Test with un-equal variances
  
  + Paired T-Test (one-sample t-test on differences)


##  Two-Sample T-Test (Create & Plot Data)
```{r echo = TRUE}
Samp1 <- rnorm(300, mean = 2.5, sd = 1)
Samp2 <- rnorm(500, mean = 3.0, sd = 1) # notice: not the same sample size
plot(density(Samp1), col="red", main="Densities of Samp1 and Samp2", xlab="")
abline(v = mean(Samp1), col = "red", lwd = 2, lty=2)
lines(density(Samp2), col="blue")
abline(v = mean(Samp2), col = "blue", lwd = 2, lty = 2)
legend("topright", legend = c("Samp1", "Samp2"),
       fill = c("red","blue"), bty = "n", cex = 1.3)
```


##  Two-Sample T-Test (Un-equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2)  # default assump: unequal variances
```


##  Two-Sample T-Test (Equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2, var.equal = TRUE)  # default assump: unequal variances
```


##  Two-Sample T-Test (Paired T-Test)
Let $D \equiv \{x_i - y_i : i=1, \ldots, n \}$, then

Null hypothesis: $\mu_D = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2[1:300], paired = TRUE) # must be of the same sample size
```


## Linear Regression Data Set Description
Here we use the "Prestige" dataset from the `car` package, which has the following variables

+ education: Average education of occupational incumbents, years, in 1971.

+ income: Average income of incumbents, dollars, in 1971.

+ women: Percentage of incumbents who are women.

+ prestige: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.

+ census: Canadian Census occupational code.

+ type: Type of occupation, a factor with levels 

    + bc: Blue Collar
    + prof: Professional, Managerial, and Technical
    + wc: White Collar


## Load Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
prestige <- read.csv(file="./data/prestige_v2.csv")
summary(prestige)
```


## Examine Observations with NA values
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
Prestige[is.na(Prestige$type),]
```

Find their row indices and corresponding names...
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
ind <- which(is.na(Prestige$type))  # gives index numbers of the NAs in the vector
rbind(index=ind, name=rownames(Prestige)[ind])  # print index with rowname
```


## Recode/Drop NA Values
Let's recode newsboys, babysitters, and farmers as blue collar and exclude athletes.

Use `rep()` to create a vector with 3 elements of "bc"
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
ind.ch <- ind[-1]
Prestige[ind.ch,"type"] <- rep("bc", 3)
summary(Prestige$type)
```

Exclude athletes...
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
Prestige <- na.omit(Prestige) 
summary(Prestige$type)
```


## Plotting - Histograms
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
hist(Prestige$prestige, col = "grey", 
     main = "Histogram of Prestige Score", xlab = "Prestige Score")
```


## Plotting - Basic Scatter Plot
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
plot(Prestige$education, Prestige$prestige,
     main = "Prestige Score by Education",
     xlab = "Ave. Years of Education", ylab = "Prestige Score")
```


## Plotting - Including Regression/Smoother Lines
`abline()` adds a line to the plot, and `lm()` is a list object that contains regression coefficients.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
plot(Prestige$education, Prestige$prestige,
     main = "Prestige Score by Education",
     xlab = "Avg Years of Education", ylab = "Prestige Score")
abline(reg = lm(prestige ~ education, data = Prestige), col = "red", lwd = 2)
lines(lowess(Prestige$education, Prestige$prestige), col = "blue", lty = 2, lwd = 2)
legend("topleft",legend = c("Regression Line", "Smoother"), col = c("red","blue"),
       lwd = c(2,2), lty = c(1,2), bty = "n")
```


## Plotting - Scatter Plot Matrix
Use the `scatterplotMatrix()` function from the `car` package
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
# Use direct ordering of the varaibles to control how they are plotted
scatterplotMatrix(Prestige[,c("prestige","education","income","women")])
```


## Plotting - Boxplots 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
boxplot(prestige ~ type, data = Prestige, col = "grey",
        main = "Distribution of Prestige Score by Types",
        xlab = "Occupation Types", ylab = "Prestige Score")
```


## Linear Regression - Fit the Model
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg <- lm(prestige ~ education + income + women, data = Prestige)
myReg
names(myReg)
```


##  Linear Regression - Summary of Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(myReg)
```


##  Linear Regression - "summary" Contents
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
sum.myReg = summary(myReg)
names(sum.myReg) # show different contents

names(myReg) # this is what we had previously
```


##  Linear Regression - Confidence Interval
+ 95% confidence interval for coefficient of 'income'
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, 'income', level=0.95)
```

+ 95% confidence interval for each coefficient
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, level=0.95)
```


##  Linear Regression - Adding Variables
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod = update(myReg, ~ . + type); summary(mod)
```


##  Linear Regression - Relevel a Factor
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
levels(Prestige$type)
Prestige$type = relevel(Prestige$type, "prof")

levels(Prestige$type)
```


##  Linear Regression - Relevel a Factor
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod = update(myReg, ~ . + type); summary(mod)
```


##  Linear Regression - Diagnostics
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6.5, fig.width=8}
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))
plot(myReg)
```


##  Linear Regression - Predict
Predict the output for a new input
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
newData = data.frame(education=13.2, income=12000, women=12)
predict(myReg, newData, interval="predict")
```


## End of Session 4
BREAK!



